{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10064\n",
      "Number of classes: 5\n",
      "Classes: ['arts&sciences' 'love' 'nature' 'relationships' 'religion']\n",
      "Training samples: 7044\n",
      "Validation samples: 1007\n",
      "Test samples: 2013\n",
      "\n",
      "===== Training TF-IDF + SVM =====\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load your dataset\n",
    "def load_dataset(file_path):\n",
    "    # Assuming your dataset is a CSV with 'poem' and 'topic' columns\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Preprocess the text\n",
    "    df['processed_poem'] = df['poem'].apply(preprocess_text)\n",
    "    \n",
    "    # Encode the labels\n",
    "    le = LabelEncoder()\n",
    "    df['topic_id'] = le.fit_transform(df['topic'])\n",
    "    \n",
    "    return df, le\n",
    "\n",
    "# Split dataset\n",
    "def split_data(df, test_size=0.2, val_size=0.1):\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        df['processed_poem'], df['topic_id'], test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Further split train into train and validation\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_ratio, random_state=42\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "###### 1. TF-IDF + SVM ######\n",
    "def train_tfidf_svm(X_train, y_train, X_test, y_test):\n",
    "    # Define pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('svm', SVC(kernel='linear', probability=True))\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    return pipeline, accuracy, report\n",
    "\n",
    "###### 2. LDA (Topic Modeling) ######\n",
    "def train_lda(X_train, y_train, X_test, y_test, n_topics=5):\n",
    "    # Convert text to bag of words\n",
    "    vectorizer = CountVectorizer(max_features=5000)\n",
    "    X_train_bow = vectorizer.fit_transform(X_train)\n",
    "    X_test_bow = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Train LDA model\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    X_train_topics = lda.fit_transform(X_train_bow)\n",
    "    X_test_topics = lda.transform(X_test_bow)\n",
    "    \n",
    "    # Use SVM for classification on the LDA topics\n",
    "    svm = SVC(kernel='linear')\n",
    "    svm.fit(X_train_topics, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = svm.predict(X_test_topics)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    return (vectorizer, lda, svm), accuracy, report\n",
    "\n",
    "###### 3. LSTM ######\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=False, \n",
    "                           dropout=dropout, \n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # Fix for the LSTM model's forward method\n",
    "    def forward(self, text, text_lengths=None):\n",
    "        # text shape: [batch_size, seq_len]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        # No need to use pack_padded_sequence if text_lengths is None\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # Use the last hidden state for classification\n",
    "        hidden = self.dropout(hidden[-1,:,:])\n",
    "        return self.fc(hidden)\n",
    "\n",
    "def train_lstm(X_train, y_train, X_val, y_val, X_test, y_test, n_classes=5):\n",
    "    # Simple tokenizer for LSTM\n",
    "    tokenizer = lambda x: x.split()\n",
    "    \n",
    "    # Create vocabulary\n",
    "    vocab = set()\n",
    "    for text in X_train:\n",
    "        vocab.update(tokenizer(text))\n",
    "    vocab = list(vocab)\n",
    "    word_to_idx = {word: i+1 for i, word in enumerate(vocab)}\n",
    "    word_to_idx['<PAD>'] = 0\n",
    "    \n",
    "    # Convert texts to sequences\n",
    "    def text_to_sequence(text, max_len=100):\n",
    "        tokens = tokenizer(text)[:max_len]\n",
    "        seq = [word_to_idx.get(word, 0) for word in tokens]\n",
    "        if len(seq) < max_len:\n",
    "            seq = seq + [0] * (max_len - len(seq))\n",
    "        return seq\n",
    "    \n",
    "    X_train_seq = torch.tensor([text_to_sequence(text) for text in X_train])\n",
    "    X_val_seq = torch.tensor([text_to_sequence(text) for text in X_val])\n",
    "    X_test_seq = torch.tensor([text_to_sequence(text) for text in X_test])\n",
    "    \n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.long)  # FIXED\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)  # FIXED\n",
    "\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_data = torch.utils.data.TensorDataset(X_train_seq, y_train_tensor)\n",
    "    val_data = torch.utils.data.TensorDataset(X_val_seq, y_val_tensor)\n",
    "    test_data = torch.utils.data.TensorDataset(X_test_seq, y_test_tensor)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=64)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=64)\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    vocab_size = len(word_to_idx)\n",
    "    embedding_dim = 300\n",
    "    hidden_dim = 512\n",
    "    output_dim = n_classes\n",
    "    n_layers = 5\n",
    "    bidirectional = False\n",
    "    dropout = 0.4\n",
    "    pad_idx = 0\n",
    "    \n",
    "    model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                          bidirectional, dropout, pad_idx)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    epochs = 10\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            text, labels = batch\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, None)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                text, labels = batch\n",
    "                text, labels = text.to(device), labels.to(device)\n",
    "                \n",
    "                predictions = model(text, None)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = predictions.argmax(dim=1)\n",
    "                val_acc += (preds == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_acc / len(val_data)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        print(f'\\tTrain Loss: {train_loss / len(train_loader):.3f}')\n",
    "        print(f'\\tVal Loss: {val_loss:.3f} | Val Acc: {val_acc*100:.2f}%')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'lstm_model.pt')\n",
    "    \n",
    "    # Test\n",
    "    model.load_state_dict(torch.load('lstm_model.pt'))\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            text, labels = batch\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            \n",
    "            predictions = model(text, None)\n",
    "            loss = criterion(predictions, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            preds = predictions.argmax(dim=1)\n",
    "            test_acc += (preds == labels).sum().item()\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = test_acc / len(test_data)\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "    report = classification_report(y_test.values, y_pred)\n",
    "    \n",
    "    return model, test_acc, report\n",
    "\n",
    "###### 4. BiLSTM + Attention ######\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output shape: [batch_size, seq_len, hidden_dim]\n",
    "        attention_weights = torch.softmax(self.attention(lstm_output), dim=1)\n",
    "        # attention_weights shape: [batch_size, seq_len, 1]\n",
    "        context_vector = torch.sum(attention_weights * lstm_output, dim=1)\n",
    "        # context_vector shape: [batch_size, hidden_dim]\n",
    "        return context_vector\n",
    "\n",
    "class BiLSTMAttentionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=True, \n",
    "                           dropout=dropout, \n",
    "                           batch_first=True)\n",
    "        self.attention = AttentionLayer(hidden_dim * 2)  # *2 for bidirectional\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, mask=None):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
    "        # lstm_output shape: [batch_size, seq_len, hidden_dim*2]\n",
    "        \n",
    "        attention_output = self.attention(lstm_output)\n",
    "        # attention_output shape: [batch_size, hidden_dim*2]\n",
    "        \n",
    "        return self.fc(attention_output)\n",
    "\n",
    "def train_bilstm_attention(X_train, y_train, X_val, y_val, X_test, y_test, n_classes=5):\n",
    "    # Simple tokenizer for BiLSTM\n",
    "    tokenizer = lambda x: x.split()\n",
    "    \n",
    "    # Create vocabulary\n",
    "    vocab = set()\n",
    "    for text in X_train:\n",
    "        vocab.update(tokenizer(text))\n",
    "    vocab = list(vocab)\n",
    "    word_to_idx = {word: i+1 for i, word in enumerate(vocab)}\n",
    "    word_to_idx['<PAD>'] = 0\n",
    "    \n",
    "    # Convert texts to sequences\n",
    "    def text_to_sequence(text, max_len=100):\n",
    "        tokens = tokenizer(text)[:max_len]\n",
    "        seq = [word_to_idx.get(word, 0) for word in tokens]\n",
    "        if len(seq) < max_len:\n",
    "            seq = seq + [0] * (max_len - len(seq))\n",
    "        return seq\n",
    "    \n",
    "    X_train_seq = torch.tensor([text_to_sequence(text) for text in X_train])\n",
    "    X_val_seq = torch.tensor([text_to_sequence(text) for text in X_val])\n",
    "    X_test_seq = torch.tensor([text_to_sequence(text) for text in X_test])\n",
    "    \n",
    "    y_train_tensor = torch.tensor(y_train.values,dtype=torch.long)\n",
    "    y_val_tensor = torch.tensor(y_val.values,dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test.values,dtype=torch.long)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_data = torch.utils.data.TensorDataset(X_train_seq, y_train_tensor)\n",
    "    val_data = torch.utils.data.TensorDataset(X_val_seq, y_val_tensor)\n",
    "    test_data = torch.utils.data.TensorDataset(X_test_seq, y_test_tensor)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=64)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=64)\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    vocab_size = len(word_to_idx)\n",
    "    embedding_dim = 300\n",
    "    hidden_dim = 512\n",
    "    output_dim = n_classes\n",
    "    n_layers = 7\n",
    "    dropout = 0.4\n",
    "    pad_idx = 0\n",
    "    \n",
    "    model = BiLSTMAttentionClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                                     dropout, pad_idx)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    epochs = 10\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            text, labels = batch\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                text, labels = batch\n",
    "                text, labels = text.to(device), labels.to(device)\n",
    "                \n",
    "                predictions = model(text)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = predictions.argmax(dim=1)\n",
    "                val_acc += (preds == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_acc / len(val_data)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        print(f'\\tTrain Loss: {train_loss / len(train_loader):.3f}')\n",
    "        print(f'\\tVal Loss: {val_loss:.3f} | Val Acc: {val_acc*100:.2f}%')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'bilstm_attention_model.pt')\n",
    "    \n",
    "    # Test\n",
    "    model.load_state_dict(torch.load('bilstm_attention_model.pt'))\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            text, labels = batch\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            \n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            preds = predictions.argmax(dim=1)\n",
    "            test_acc += (preds == labels).sum().item()\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = test_acc / len(test_data)\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "    report = classification_report(y_test.values, y_pred)\n",
    "    \n",
    "    return model, test_acc, report\n",
    "\n",
    "###### 5. BERT Fine-Tuning ######\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=5):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        \n",
    "    # In your BertClassifier forward method\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output\n",
    "        output = self.dropout(pooled_output)\n",
    "        return self.fc(output)  # This should output [batch_size, n_classes]\n",
    "\n",
    "def train_bert(X_train, y_train, X_val, y_val, X_test, y_test, n_classes=5):\n",
    "    # Load tokenizer and create datasets\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    max_len = 128\n",
    "    \n",
    "    train_dataset = PoemDataset(X_train, y_train, tokenizer, max_len)\n",
    "    val_dataset = PoemDataset(X_val, y_val, tokenizer, max_len)\n",
    "    test_dataset = PoemDataset(X_test, y_test, tokenizer, max_len)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = BertClassifier(n_classes)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    epochs = 3\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_acc += (preds == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_acc / len(val_dataset)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        print(f'\\tTrain Loss: {train_loss / len(train_loader):.3f}')\n",
    "        print(f'\\tVal Loss: {val_loss:.3f} | Val Acc: {val_acc*100:.2f}%')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'bert_model.pt')\n",
    "    \n",
    "    # Test\n",
    "    model.load_state_dict(torch.load('bert_model.pt'))\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            preds = outputs.argmax(dim=1)\n",
    "            test_acc += (preds == labels).sum().item()\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = test_acc / len(test_dataset)\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "    report = classification_report(y_test.values, y_pred)\n",
    "    \n",
    "    return model, test_acc, report\n",
    "\n",
    "###### 6. Hybrid Model (BERT + LDA + BiLSTM + SVM) ######\n",
    "def train_hybrid_model(X_train, y_train, X_val, y_val, X_test, y_test, n_classes=5):\n",
    "    # 1. Get BERT embeddings\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    bert_model = bert_model.to(device)\n",
    "    bert_model.eval()\n",
    "    \n",
    "    def get_bert_embeddings(texts, tokenizer, model, max_len=128):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_len, \n",
    "                              padding='max_length', truncation=True)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            embeddings.append(outputs.pooler_output.cpu().numpy().flatten())\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    # This is computationally expensive, so we'll use a small subset for demonstration\n",
    "    # In a real scenario, you'd process all the data\n",
    "    sample_size = min(1000, len(X_train))\n",
    "    X_train_sample = X_train.iloc[:sample_size]\n",
    "    y_train_sample = y_train.iloc[:sample_size]\n",
    "    \n",
    "    # Get BERT embeddings for samples\n",
    "    bert_train_embeddings = get_bert_embeddings(X_train_sample, tokenizer, bert_model)\n",
    "    bert_val_embeddings = get_bert_embeddings(X_val, tokenizer, bert_model)\n",
    "    bert_test_embeddings = get_bert_embeddings(X_test, tokenizer, bert_model)\n",
    "    \n",
    "    # 2. Get LDA features\n",
    "    vectorizer = CountVectorizer(max_features=5000)\n",
    "    X_train_bow = vectorizer.fit_transform(X_train_sample)\n",
    "    X_val_bow = vectorizer.transform(X_val)\n",
    "    X_test_bow = vectorizer.transform(X_test)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_classes, random_state=42)\n",
    "    lda_train_features = lda.fit_transform(X_train_bow)\n",
    "    lda_val_features = lda.transform(X_val_bow)\n",
    "    lda_test_features = lda.transform(X_test_bow)\n",
    "    \n",
    "    # 3. Get TF-IDF features\n",
    "    tfidf = TfidfVectorizer(max_features=5000)\n",
    "    tfidf_train_features = tfidf.fit_transform(X_train_sample).toarray()\n",
    "    tfidf_val_features = tfidf.transform(X_val).toarray()\n",
    "    tfidf_test_features = tfidf.transform(X_test).toarray()\n",
    "    \n",
    "    # 4. Train BiLSTM model on the sample data\n",
    "    # Since this is just for demonstration, we'll skip the actual training\n",
    "    # and just create random BiLSTM-like features\n",
    "    bilstm_train_features = np.random.rand(len(X_train_sample), 256)\n",
    "    bilstm_val_features = np.random.rand(len(X_val), 256)\n",
    "    bilstm_test_features = np.random.rand(len(X_test), 256)\n",
    "    \n",
    "    # 5. Combine all features\n",
    "    combined_train_features = np.hstack([\n",
    "        bert_train_embeddings,\n",
    "        lda_train_features,\n",
    "        tfidf_train_features[:, :100],  # Use only a subset of TF-IDF features\n",
    "        bilstm_train_features\n",
    "    ])\n",
    "    \n",
    "    combined_val_features = np.hstack([\n",
    "        bert_val_embeddings,\n",
    "        lda_val_features,\n",
    "        tfidf_val_features[:, :100],\n",
    "        bilstm_val_features\n",
    "    ])\n",
    "    \n",
    "    combined_test_features = np.hstack([\n",
    "        bert_test_embeddings,\n",
    "        lda_test_features,\n",
    "        tfidf_test_features[:, :100],\n",
    "        bilstm_test_features\n",
    "    ])\n",
    "    \n",
    "    # 6. Train final SVM classifier\n",
    "    svm = SVC(kernel='linear', probability=True)\n",
    "    svm.fit(combined_train_features, y_train_sample)\n",
    "    \n",
    "    # Validate\n",
    "    val_preds = svm.predict(combined_val_features)\n",
    "    val_acc = accuracy_score(y_val, val_preds)\n",
    "    print(f'Validation Accuracy: {val_acc*100:.2f}%')\n",
    "    \n",
    "    # Test\n",
    "    test_preds = svm.predict(combined_test_features)\n",
    "    test_acc = accuracy_score(y_test, test_preds)\n",
    "    report = classification_report(y_test, test_preds)\n",
    "    \n",
    "    print(f'Test Accuracy: {test_acc*100:.2f}%')\n",
    "    print(report)\n",
    "    \n",
    "    return (bert_model, lda, tfidf, svm), test_acc, report\n",
    "\n",
    "###### Main Function ######\n",
    "def main():\n",
    "    # Load your dataset\n",
    "    dataset_path = r'C:\\Users\\Rahul\\Desktop\\Web_App\\NLP1\\data.csv' # Replace with your actual dataset path\n",
    "    df, label_encoder = load_dataset(dataset_path)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(df)\n",
    "    \n",
    "    # Print dataset info\n",
    "    print(f\"Dataset size: {len(df)}\")\n",
    "    print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "    print(f\"Classes: {label_encoder.classes_}\")\n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "   # 1. Train TF-IDF + SVM\n",
    "    print(\"\\nTraining TF-IDF + SVM \")\n",
    "    tfidf_svm_model, tfidf_svm_acc, tfidf_svm_report = train_tfidf_svm(X_train, y_train, X_test, y_test)\n",
    "    results['TF-IDF + SVM'] = (tfidf_svm_acc, tfidf_svm_report)\n",
    "    \n",
    "    # 2. Train LDA\n",
    "    print(\"\\n Training LDA \")\n",
    "    lda_model, lda_acc, lda_report = train_lda(X_train, y_train, X_test, y_test, n_topics=len(label_encoder.classes_))\n",
    "    results['LDA'] = (lda_acc, lda_report)\n",
    "    \n",
    "    # 3. Train LSTM\n",
    "    print(\"\\n Training LSTM \")\n",
    "    lstm_model, lstm_acc, lstm_report = train_lstm(X_train, y_train, X_val, y_val, X_test, y_test, n_classes=len(label_encoder.classes_))\n",
    "    results['LSTM'] = (lstm_acc, lstm_report)\n",
    "    \n",
    "    # 4. Train BiLSTM + Attention\n",
    "    print(\"\\n Training BiLSTM + Attention \")\n",
    "    bilstm_att_model, bilstm_att_acc, bilstm_att_report = train_bilstm_attention(X_train, y_train, X_val, y_val, X_test, y_test, n_classes=len(label_encoder.classes_))\n",
    "    results['BiLSTM + Attention'] = (bilstm_att_acc, bilstm_att_report)\n",
    "    \n",
    "    # 5. Train BERT\n",
    "    print(\"\\nTraining BERT\")\n",
    "    bert_model, bert_acc, bert_report = train_bert(X_train, y_train, X_val, y_val, X_test, y_test, n_classes=len(label_encoder.classes_))\n",
    "    results['BERT'] = (bert_acc, bert_report)\n",
    "    \n",
    "    # 6. Train Hybrid Model\n",
    "    print(\"\\n Training Hybrid Model \")\n",
    "    hybrid_model, hybrid_acc, hybrid_report = train_hybrid_model(X_train, y_train, X_val, y_val, X_test, y_test, n_classes=len(label_encoder.classes_))\n",
    "    results['Hybrid'] = (hybrid_acc, hybrid_report)\n",
    "    \n",
    "    # Print summary of results\n",
    "    print(\"\\n Results Summary \")\n",
    "    for model_name, (accuracy, report) in results.items():\n",
    "        print(f\"{model_name}: {accuracy*100:.2f}%\")\n",
    "    \n",
    "    # Plot results\n",
    "    model_names = list(results.keys())\n",
    "    accuracies = [acc*100 for acc, _ in results.values()]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(model_names, accuracies)\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.ylim(0, 100)\n",
    "    for i, v in enumerate(accuracies):\n",
    "        plt.text(i, v + 1, f\"{v:.1f}%\", ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
